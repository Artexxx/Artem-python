{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of Words –∏–ª–∏ –º–µ—à–æ–∫ —Å–ª–æ–≤ ‚Äî  –∏—Å–ø–æ–ª—å–∑—É–µ–µ—Ç—Å—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Å–ª–æ–≤, –≤—Ö–æ–¥—è—â–∏—Ö –≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç.\n",
    "\n",
    "–ß–∞—Å—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –≤ –≤–∏–¥–µ –º–∞—Ç—Ä–∏—Ü—ã, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É, –∞ —Å—Ç–æ–ª–±—Ü—ã ‚Äî –≤—Ö–æ–¥—è—â–∏–µ –≤ –Ω–µ–≥–æ —Å–ª–æ–≤–∞. –Ø—á–µ–π–∫–∏ –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ —è–≤–ª—è—é—Ç—Å—è —á–∏—Å–ª–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç. –î–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–¥–æ–±–Ω–∞ —Ç–µ–º, —á—Ç–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ —Å–ª–æ–≤ –≤ –ø–æ–Ω—è—Ç–Ω—ã–π –¥–ª—è –∫–æ–º–ø—å—Ç–µ—Ä–∞ —è–∑—ã–∫ —Ü–∏—Ñ—Ä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "raw_documents = ['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two']\n",
    "\n",
    "bag = count.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency TF\n",
    "tf$(t, d)$ —á–∞—Å—Ç–æ—Ç–∞ —Ç–µ—Ä–º–∏–Ω–∞ t –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ  d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and        0\n",
       "is         1\n",
       "one        2\n",
       "shining    3\n",
       "sun        4\n",
       "sweet      5\n",
       "the        6\n",
       "two        7\n",
       "weather    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = pd.Series(count.vocabulary_).sort_values()\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>shining</th>\n",
       "      <th>sun</th>\n",
       "      <th>sweet</th>\n",
       "      <th>the</th>\n",
       "      <th>two</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  is  one  shining  sun  sweet  the  two  weather\n",
       "0    0   1    0        1    1      0    1    0        0\n",
       "1    0   1    0        0    0      1    1    0        1\n",
       "2    2   3    2        1    1      1    2    1        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag.toarray(), columns=lexicon.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF   \n",
    "term frequency - inverse document frequency\n",
    "<br>—á–∞—Å—Ç–æ—Ç–∞ —Ç–µ—Ä–º–∞ - –æ–±—Ä–∞—Ç–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    ">–º–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤–µ—Å–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–∞—Ö\n",
    "\n",
    "**tf-idf** —ç—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã —Ç–µ—Ä–º–∞ –Ω–∞ –æ–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "\n",
    "*tf-idf* $(t, d) = \\mathrm{tf}(t, d)*\\mathrm{idf}(t,d)$ \n",
    "<br>${\\displaystyle \\mathrm {idf} (t,d)=\\log {\\frac{N_d}{1+ \\mathrm {df}(d, t)}}}$\n",
    "-  $N_d$ - –æ–±—â–µ–µ –∫–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "-  $ \\mathrm {df}(d, t)$ -  –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—à–∏—Ö —Ç–µ—Ä–º "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>shining</th>\n",
       "      <th>sun</th>\n",
       "      <th>sweet</th>\n",
       "      <th>the</th>\n",
       "      <th>two</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and    is  one  shining   sun  sweet   the   two  weather\n",
       "0  0.0  0.43  0.0     0.56  0.56   0.00  0.43  0.00     0.00\n",
       "1  0.0  0.43  0.0     0.00  0.00   0.56  0.43  0.00     0.56\n",
       "2  0.5  0.45  0.5     0.19  0.19   0.19  0.30  0.25     0.19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "pd.DataFrame(tfidf.fit_transform(count.fit_transform(raw_documents)).toarray(),\n",
    "             columns=lexicon.index).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –ø—Ä–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é L2\n",
    "<br> - –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.4, 3. , 3.4, 1.3, 1.3, 1.3, 2. , 1.7, 1.3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(raw_documents)).toarray()[-1]\n",
    "raw_tfidf.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{\\text{norm}} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.4, 0.5, 0.2, 0.2, 0.2, 0.3, 0.3, 0.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "**–ó–∞–¥–∞—á–∞:**\n",
    "    —É–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –∫—Ä–æ–º–µ —Å–º–∞–π–ª–æ–≤ :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(raw_text):\n",
    "    \"\"\" \n",
    "    –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç.\n",
    "    1. —É–¥–∞–ª–µ–Ω–∏–µ html\n",
    "    2. –ø–æ–∏—Å–∫ —ç–º–æ—Ü–∏–π\n",
    "    3. —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –∑–Ω–∞–∫–æ–≤ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É + —ç–º–æ—Ü–∏–∏\n",
    "    \"\"\"\n",
    "    # cleantext = BeautifulSoup(raw_text, \"lxml\").text\n",
    "    cleantext = re.sub('<[^>]*>', '', raw_text)  # ________________________ –ø–∞—Ä—Å–∏–Ω–≥ html —Å re –±—ã—Å—Ç—Ä–æ, –Ω–æ –ø–ª–æ—Ö–æ\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', cleantext)\n",
    "    text = (re.sub('[\\W]+', ' ', cleantext.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ~29.6s, —Å –ø–æ–º–æ—â—å—é re ~7s \n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word stemming\n",
    "-—Å–æ–∫—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞ –¥–æ –∏—Ö –∫–æ—Ä–Ω–µ–≤–æ–π —Ñ–æ—Ä–º—ã\n",
    "> **–õ–µ–º–º–∞—Ç–∏–∑–∞ÃÅ—Ü–∏—è** ‚Äî –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã –∫ –ª–µ–º–º–µ ‚Äî –µ—ë –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π (—Å–ª–æ–≤–∞—Ä–Ω–æ–π) —Ñ–æ—Ä–º–µ.\n",
    "> <br>* –±–æ–ª–µ–µ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    \"\"\" –ê–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–µ–º–º–∏–Ω–≥–∞ –ü–æ—Ä—Ç–µ—Ä–∞\"\"\"\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/artem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english') # 179 —Å–ª–æ–≤\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
    "    if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test  = df.loc[25000:, 'review'].values\n",
    "y_test  = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words' : [stop, None],\n",
    "                'vect__tokenizer' : [tokenizer, tokenizer_porter],\n",
    "                   'clf__penalty' : ['l1', 'l2'],\n",
    "                        'clf__C'  : [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words' : [stop, None],\n",
    "                'vect__tokenizer' : [tokenizer, tokenizer_porter],\n",
    "                  'vect__use_idf' : [False],\n",
    "                     'vect__norm' : [None],\n",
    "                   'clf__penalty' : ['l1', 'l2'],\n",
    "                         'clf__C' : [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=1))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–æ–∂–Ω–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ 93%, –Ω–æ –Ω–∞ —ç—Ç–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è ~10 –º–∏–Ω—É—Ç\n",
    "—É–±–∏—Ç—å_–∫–æ–º–ø_—Ä–µ—à—ë—Ç—á–∞—Ç—ã–º_–ø–æ–∏—Å–∫–æ–º = False\n",
    "\n",
    "if  —É–±–∏—Ç—å_–∫–æ–º–ø_—Ä–µ—à—ë—Ç—á–∞—Ç—ã–º_–ø–æ–∏—Å–∫–æ–º:\n",
    "    gs_lr_tfidf.fit(X_train, y_train)\n",
    "    print(f'\\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {gs_lr_tfidf.best_params_}')\n",
    "    print(f'\\n–õ—É—á—à–∏–π –†–µ–∑—É–ª—å—Ç–∞—Ç: {gs_lr_tfidf.best_score_:.2}')\n",
    "    clf = gs_lr_tfidf.best_estimator_\n",
    "    print(f'–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:{clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
      "–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 89.212%\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf.fit(X_train, y_train)\n",
    "score = lr_tfidf.score(X_test, y_test)\n",
    "print(f'–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\\n–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {score:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í–Ω–µ—à–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    \"\"\" –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, —á–∏—Ç–∞–µ—Ç –∏ –≤–æ–∑—Ä–∞—â–∞–µ—Ç –ø–æ 1 –¥–æ–∫—É–º–µ–Ω—Ç—É –∏–∑ —Ñ–∞–π–ª–∞\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label = next(stream_docs(path = 'movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size= 5000):\n",
    "    \"\"\" –ù–∞–∫–∞–ª–∏–≤–∞–µ—Ç –ø–æ—Ç–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ size –∏ –≤–æ–∑—Ä–∞—â–∞–µ—Ç \"\"\"\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, # n- –∫–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "                         preprocessor=None, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [üôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇüôÇ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:10\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45, bar_char= 'üôÇ')\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–±–æ—Ç–∞ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ–Ω–ª–∞–π–Ω-–∞–ª–≥–æ—Ä–∏—Ç–º\n",
      "–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 80.720%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test_raw, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test_raw)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f'–†–∞–±–æ—Ç–∞ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ–Ω–ª–∞–π–Ω-–∞–ª–≥–æ—Ä–∏—Ç–º\\n–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {score:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing estimators\n",
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf,  open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('movieclassifier')\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: positive\n",
      "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 79.20%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {label[clf.predict(X)[0]]}\\n–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {np.max(clf.predict_proba(X)):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    ">**–ó–∞–¥–∞—á–∞:**<br>\n",
    "–ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫ –∫–∞–∫–∏–º —Ç–µ–º–∞–º –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–∞–∂–¥—ã–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –†–∞–∑–º–µ—â–µ–Ω–∏–µ –î–∏—Ä–∏—Ö–ª–µ (LDA)\n",
    "\n",
    "**–ò–¥–µ—è:**<br>–æ—Ç—ã—Å–∫–∞—Ç—å –≥—Ä—É–ø–ø—ã —Å–ª–æ–≤, —á–∞—Å—Ç–æ –ø–æ—è–≤–ª—è—é—â–∏–µ—Å—è –≤–º–µ—Å—Ç–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "        <br>- —ç—Ç–∏ —Å–ª–æ–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ç–µ–º—ã \n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º:** \n",
    "    <br>LDA –ø–æ–ª—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤ –∏ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç –µ—ë –Ω–∞ 2 –º–∞—Ç—Ä–∏—Ü—ã\n",
    "    <br>- –º–∞—Ç—Ä–∏—Ü–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–ª–æ–≤ –Ω–∞ —Ç–µ–º—ã\n",
    "    <br>- –º–∞—Ç—Ä–∏—Ü–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ç–µ–º—ã\n",
    "    \n",
    "  \n",
    "\n",
    "<img src='https://user-images.githubusercontent.com/54672403/82434974-503d6d00-9a9c-11ea-8f42-e41a34325106.png' width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42, learning_method='batch',n_jobs=-1)\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–º–∞: 1\n",
      "\ttv school dvd watched women\n",
      "–¢–µ–º–∞: 2\n",
      "\trole music performance actor play\n",
      "–¢–µ–º–∞: 3\n",
      "\tbook novel murder version read\n",
      "–¢–µ–º–∞: 4\n",
      "\thorror sex gore thriller budget\n",
      "–¢–µ–º–∞: 5\n",
      "\tguy worst stupid game minutes\n",
      "–¢–µ–º–∞: 6\n",
      "\tfather wife family mother john\n",
      "–¢–µ–º–∞: 7\n",
      "\tcomedy series kids episode fun\n",
      "–¢–µ–º–∞: 8\n",
      "\tfeel audience documentary different cinema\n",
      "–¢–µ–º–∞: 9\n",
      "\twar american men police country\n",
      "–¢–µ–º–∞: 10\n",
      "\taction effects special budget original\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"–¢–µ–º–∞: {topic_idx + 1}\")\n",
    "    print('\\t'+\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
